p1a
import pandas as pd
df=pd.read_csv("/content/SampleData.csv")
print(df)
print(df.info())
print(df["Name"])
print(df["Name"].isnull())
print(df["Name"])
df.fillna("null",inplace=True)
print(df["Name"])
print(df["Name"].isnull())
df["Name"]=df["Name"].str.replace("null","Shipra")
print(df["Name"])
df["Name"]=df["Name"].str.lower()
print(df["Name"])
df['Name']=df['Name'].str.strip()
print(df['Name'])
import pandas as pd
df={'Value':[10,26,80,34,27,27,19,38,28]}
df=pd.DataFrame(df)
print(df)
Q1=df['Value'].quantile(0.25)
Q3=df['Value'].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR
outliers=df[(df['Value']<lower_bound) | (df['Value']>upper_bound)]
print("Outliers:\n",outliers)

p1b
import pandas as pd
df=pd.read_csv("/content/train.csv")
print(df)
print(df.describe())
print(df.describe(include='all'))
x = df.drop(["Survived"], axis=1)
y = df["Survived"]
print("Features :\n", x)
print("Targets :\n", y)
import matplotlib.pyplot as plt
df.hist()
plt.show()
import matplotlib.pyplot as plt
df.plot.bar()
plt.bar(df['Age'], df['Pclass'])
plt.xlabel("Age")
plt.ylabel("Pclass")
plt.title("KSMSCIT014 - SHIPRA JANA")
plt.show()
import matplotlib.pyplot as plt
plt.scatter(df['Pclass'], df['Age'])
plt.xlabel("Pclass")
plt.ylabel("Age")
plt.title("KSMSCIT014 - SHIPRA JANA")
plt.show()
import matplotlib.pyplot as plt
import seaborn as sns
sns.boxplot(x='Pclass', y='Age', data=df)
plt.xlabel("Pclass")
plt.ylabel("Age")
plt.title("KSMSCIT014 - SHIPRA JANA")
plt.show()

p1c
import numpy as np
import pandas as pd
df=pd.read_csv("/content/iris.csv")
print(df['species'].unique
from sklearn import preprocessing
le=preprocessing.LabelEncoder()
df['species']=le.fit_transform(df['species'])
print(df['species'].unique())
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import Binarizer
data = load_iris(as_frame=True)
df = data.frame
print(df['sepal length (cm)'])
binarizer = Binarizer(threshold=5)
df['sepal length (cm) binary'] = binarizer.fit_transform(df[['sepal length (cm)']])
print(df['sepal length (cm) binary'])

p2a
import numpy as np
import pandas as pd
data=pd.read_csv("/content/iris.csv")
data.head()
x = data.drop('species', axis=1)
y = data['species']
print("Features")
print(x)
print("Target")
print(y)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print("Training Data")
print(x_train)
print(y_train)
print("Testing Data")
print(x_test)
print(y_test)
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
print(y_pred)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10, 6))
plot_tree(model, feature_names=x.columns, class_names=model.classes_, filled=True, rounded=True)
plt.title("\n\nKSMSCIT014 - Shipra Jana\n\n")
plt.show()

p2b
from sklearn.datasets import load_iris
data = load_iris()
x = data.data
y = data.target
print("x : \n", x)
print("y : \n", y)
model = DecisionTreeClassifier(criterion='gini', random_state=42)
model.fit(x, y)
test_data = [[6.1,3.4,9,1.8]]
prediction = model.predict(test_data)
print("Prediction : \n", prediction)
tree_rules = export_text(model, feature_names=data.feature_names)
print(tree_rules)
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 8))
plot_tree(model, filled=True, feature_names=data.feature_names, class_names=data.target_names)
plt.title("\n\nKSMSCIT 014 - SHIPRA JANA \n\nDecision Tree using Gini Index")
plt.show()

p2c
parent_node=[50,30,20]
child_1=[30,20,10]
child_2=[20,10,10]
print("Parent Node4 : ",parent_node)
print("Child Node1 : ",child_1)
print("Child Node2 : ",child_2)
def gini_index(classes):
  total = sum(classes)
  gini = 1
  for c in classes:
    gini -= (c/total)**2
  return gini
gini_parent = gini_index(parent_node)
print("Parent Gini Index : \n", gini_parent)
def weighted_gini(children):
  total=sum([sum(child) for child in children])
  weighted_gini=0
  for child in children:
    weighted_gini+=(sum(child)/total)*gini_index(child)
  return weighted_gini
gini_split=weighted_gini([child_1, child_2])
print("Split Gini Index : \n", gini_split)

p2d
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
import matplotlib.pyplot as plt
data = load_iris()
X = data.data  # Features
y = data.target  # Labels
clf = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)
clf.fit(X, y)
plt.figure(figsize=(12, 8))
plot_tree(
    clf,
    feature_names=data.feature_names,
    class_names=data.target_names,
    filled=True,
    rounded=True
)
plt.title("\n\nKSMSCIT 014 - SHIPRA JANA \n\nDecision Tree using (Using Information Gain - Entropy)")
plt.show()
tree_rules = export_text(clf, feature_names=data.feature_names)
  print(tree_rules)

p2e
import numpy as np
def entropy(classes):
    total = sum(classes)
    proportions = [count / total for count in classes if count > 0]
    return -sum(p * np.log2(p) for p in proportions)
def information_gain(parent, children):
    total_instances = sum(parent)
    parent_entropy = entropy(parent)
    weighted_entropy = sum(
        (sum(child) / total_instances) * entropy(child) for child in children
    )
    return parent_entropy - weighted_entropy
parent_node = [50, 30, 20]  # Class A: 50, Class B: 30, Class C: 20
child_1 = [30, 20, 10]      # Class A: 30, Class B: 20, Class C: 10
child_2 = [20, 10, 10]      # Class A: 20, Class B: 10, Class C: 10
print("Parent Node: ",parent_node)
print("Child Node1: ",child_1)
print("Child Node2: ",child_2)
parent_entropy = entropy(parent_node)
weighted_entropy = sum([entropy(child_1), entropy(child_2)])
gain = information_gain(parent_node, [child_1, child_2])
print(f"Entropy (Parent Node): {parent_entropy:.4f}")
print(f"Weighted Entropy (After Split): {weighted_entropy:.4f}")
print(f"Information Gain: {gain:.4f}")
import numpy as np
def entropy(classes):
    total = sum(classes)
    proportions = [count / total for count in classes if count > 0]
    return -sum(p * np.log2(p) for p in proportions)

def information_gain(parent, children):
    total_instances = sum(parent)
    parent_entropy = entropy(parent)
    weighted_entropy = sum(
        (sum(child) / total_instances) * entropy(child) for child in children
    )
    return parent_entropy - weighted_entropy
parent_node = [50, 30, 20]  # Class A: 50, Class B: 30, Class C: 20
child_1 = [30, 20, 10]      # Class A: 30, Class B: 20, Class C: 10
child_2 = [20, 10, 10]      # Class A: 20, Class B: 10, Class C: 10
print("Parent Node: ",parent_node)
print("Child Node1: ",child_1)
print("Child Node2: ",child_2)
parent_entropy = entropy(parent_node)
weighted_entropy = sum([entropy(child_1), entropy(child_2)])
gain = information_gain(parent_node, [child_1, child_2])
print(f"Entropy (Parent Node): {parent_entropy:.4f}")
print(f"Weighted Entropy (After Split): {weighted_entropy:.4f}")
print(f"Information Gain: {gain:.4f}")

p3a
import numpy as np
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X = np.array([
    [1,1,1,0,0],
    [1,1,0,0,0],
    [1,0,0,0,0],
    [0,0,0,0,0],
    [0,0,1,1,1],
    [0,0,1,1,1]
])
# Labels (1: Spam, 0: Ham)
Y = np.array([1,1,1,0,0,0])
X_train , X_test , Y_train , Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)
model = BernoulliNB() 
model.fit(X_train,Y_train)
Y_pred = model.predict(X_test)
accuracy = accuracy_score(Y_test,Y_pred)
#print("Accuracy:",accuracy)
print(f"Accuracy: { accuracy * 100:.2f}%")
new_email = np.array([[1,1,0,0,0]])
prediction = model.predict(new_email)
print("When we use testing data as [1, 1,0, 0, 0]")
print("Predicted class for the new email (0 = Ham , 1 = Spam):",prediction)

p3b
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target
print("Dataset \n", iris)
